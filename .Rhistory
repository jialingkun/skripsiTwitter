runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
result.factor[[1]]
z = result.factor[[1]]
z[order(z[,"frequency"])]
z[rev(order(z[,"frequency"]))]
z[order(z[,"frequency"]),]
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
z[,"frequency"]
as.numeric(z[,"frequency"])
order(as.numeric(z[,"frequency"]))
order(as.integer(z[,"frequency"]))
z[order(z[,"frequency"]),]
z[order(as.numeric(z[,"frequency"])),]
z[rev(order(as.numeric(z[,"frequency"]))),]
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
result.factor[[i]][1,,drop=FALSE]
result.factor[[i]][1,]
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
shiny::runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
shiny::runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
shiny::runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
clean_text = function(x, topic)
{
#tolower function often return error on certain character, need try catch
try.error = function(z) #To convert the text in lowercase
{
y = NA
try_error = tryCatch(tolower(z), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(z)
return(y)
}
#case folding
x = gsub("(\r\n)+|\r+|\n+", " ", x) # remove line break
x = gsub("@\\w+", "", x) # remove at(@)
x = gsub("&[^ ]*", "", x) # remove other format (&amp)
x = gsub("http[^ ]*", "", x)  # remove http
x = iconv(x, "latin1", "ASCII", sub="") # remove emoticon
x = gsub("<[^ ]*>", "", x) # remove other format (<564+> emoticon bug)
x = gsub("[[:punct:]]", " ", x) # remove punctuation
x = gsub("[[:digit:]]", "", x) # remove numbers/Digits
x = sapply(x, try.error) #lower case
x = gsub("rt ", "", x) # remove Retweet tag
x = gsub(topic, "", x) # remove main topic term
#filtering
cStopwordID = readLines("stopword-ID.txt") #read stopword file
corpus = Corpus(VectorSource(x)) #tm package only support data type corpus
x = tm_map(corpus, removeWords, c(stopwords("english"),cStopwordID))
#case folding
x = gsub("[ |\t]{2,}", " ", x) # remove tabs
x = gsub("^ ", "", x)  # remove blank spaces at the beginning
x = gsub(" $", "", x) # remove blank spaces at the end
#tokenizing
token = str_split(x, '\\s+')
size = as.numeric(length(x))
for(i in 1:size){
token[[i]] = sapply(token[[i]], katadasaR)
token[[i]] = str_c(token[[i]],collapse=' ') #untokenize
}
return(token)
}
searchterm <- "machung" #temp for debug
filenameNegative <- c(searchterm,"TrainNegative.csv")
filenameNegative <- str_c(filenameNegative,collapse='')
filenamePositive <- c(searchterm,"TrainPositive.csv")
filenamePositive <- str_c(filenamePositive,collapse='')
filenameTest <- c(searchterm,"DataTestRaw.csv") #temp for debug
filenameTest <- str_c(filenameTest,collapse='')
library(devtools)
library(httr)
library(twitteR)
library(tm)
library(katadasaR)
library(stringr)
library(psych)
searchterm <- "machung" #temp for debug
filenameNegative <- c(searchterm,"TrainNegative.csv")
filenameNegative <- str_c(filenameNegative,collapse='')
filenamePositive <- c(searchterm,"TrainPositive.csv")
filenamePositive <- str_c(filenamePositive,collapse='')
filenameTest <- c(searchterm,"DataTestRaw.csv") #temp for debug
filenameTest <- str_c(filenameTest,collapse='')
tweetsTrain.positive<-read.csv(filenamePositive,header=T)
tweetsTrain.positive$Tweet<- clean_text(tweetsTrain.positive$Tweet, searchterm)
tweetsTrain.negative<-read.csv(filenameNegative,header=T)
tweetsTrain.negative$Tweet<- clean_text(tweetsTrain.negative$Tweet, searchterm)
tweetsTestRaw<-read.csv(filenameTest,header=T)
tweetsTest<-clean_text(tweetsTestRaw$text, searchterm)
View(tweetsTestRaw)
write.csv(tweetsTest, file=paste('machungtweetclean.csv'))
tweetsTest
tweetsTest = as.matrix(tweetsTest)
View(tweetsTest)
write.csv(tweetsTest, file=paste('machungtweetclean.csv'))
tweetsTest<-clean_text(tweetsTestRaw$text, searchterm)
# Collect data
tweetsTrain.positive<-read.csv(filenamePositive,header=T)
tweetsTrain.positive$Tweet<- clean_text(tweetsTrain.positive$Tweet, searchterm)
tweetsTrain.negative<-read.csv(filenameNegative,header=T)
tweetsTrain.negative$Tweet<- clean_text(tweetsTrain.negative$Tweet, searchterm)
tweetsTestRaw<-read.csv(filenameTest,header=T)
tweetsTest<-clean_text(tweetsTestRaw$text, searchterm)
tweetsTrain.positive["class"]<-rep("positif",nrow(tweetsTrain.positive))
tweetsTrain.negative["class"]<-rep("negatif",nrow(tweetsTrain.negative))
# Create corpus
tweetsTrain.positive.corpus <- Corpus(VectorSource(as.vector(tweetsTrain.positive$Tweet)))
tweetsTrain.negative.corpus <- Corpus(VectorSource(as.vector(tweetsTrain.negative$Tweet)))
tweetsTest.corpus <- Corpus(VectorSource(as.vector(tweetsTest)))
# Create term document matrix
tweetsTrain.positive.matrix <- t(TermDocumentMatrix(tweetsTrain.positive.corpus,control = list(wordLengths=c(3,Inf)))); #exclude word less than 3 character
tweetsTrain.negative.matrix <- t(TermDocumentMatrix(tweetsTrain.negative.corpus,control = list(wordLengths=c(3,Inf))));
tweetsTest.matrix <- t(TermDocumentMatrix(tweetsTest.corpus,control = list(wordLengths=c(3,Inf))));
# Probability Matrix
probabilityMatrix <-function(docMatrix)
{
# Sum up the term frequencies
termSums=cbind(colnames(as.matrix(docMatrix)),as.numeric(colSums(as.matrix(docMatrix))))
# Add one
termSums=cbind(termSums,as.numeric(termSums[,2])+1)
# Calculate the probabilties
termSums=cbind(termSums,(as.numeric(termSums[,3])/sum(as.numeric(termSums[,3]))))
# Calculate the natural log of the probabilities
termSums=cbind(termSums,log(as.numeric(termSums[,4])))
# Add pretty names to the columns
colnames(termSums)=c("term","count","additive","probability","lnProbability")
termSums
}
tweetsTrain.positive.pMatrix<-probabilityMatrix(tweetsTrain.positive.matrix)
tweetsTrain.negative.pMatrix<-probabilityMatrix(tweetsTrain.negative.matrix)
write.csv(tweetsTrain.positive.pMatrix, file=paste('probmatrixpositive.csv'))
getProbability <- function(testChars,probabilityMatrix)
{
charactersFound=probabilityMatrix[probabilityMatrix[,1] %in% testChars,"term"] #in is a matching function
# Count how many words were not found in the positive matrix
charactersNotFound=length(testChars)-length(charactersFound)
# Add the normalized probabilities for the words founds together
charactersFoundSum=sum(as.numeric(probabilityMatrix[probabilityMatrix[,1] %in% testChars,"lnProbability"]))
# We use ln(1/total smoothed words) for words not found
charactersNotFoundSum=charactersNotFound*log(1/sum(as.numeric(probabilityMatrix[,"additive"])))
#This is our probability
prob=charactersFoundSum+charactersNotFoundSum
prob
}
# Get the matrix
tweetsTest.matrix<-as.matrix(tweetsTest.matrix)
classified<-NULL
classprob<-NULL #temp for testing
for(documentNumber in 1:nrow(tweetsTest.matrix))
{
# Extract the test words
tweetsTest.chars<-names(tweetsTest.matrix[documentNumber,tweetsTest.matrix[documentNumber,] >= 1])
# Get the probabilities
positiveProbability <- getProbability(tweetsTest.chars,tweetsTrain.positive.pMatrix)
negativeProbability <- getProbability(tweetsTest.chars,tweetsTrain.negative.pMatrix)
# Add it to the classification list
classified<-c(classified,ifelse(positiveProbability>negativeProbability,"positive","negative"))
classprobtemp <- cbind(positiveProbability,negativeProbability)
classprob <- c(classprob, classprobtemp)
}
# A holder for classification
classified<-NULL
classprobpos<-NULL #temp for testing
classprobneg<-NULL #temp for testing
for(documentNumber in 1:nrow(tweetsTest.matrix))
{
# Extract the test words
tweetsTest.chars<-names(tweetsTest.matrix[documentNumber,tweetsTest.matrix[documentNumber,] >= 1])
# Get the probabilities
positiveProbability <- getProbability(tweetsTest.chars,tweetsTrain.positive.pMatrix)
negativeProbability <- getProbability(tweetsTest.chars,tweetsTrain.negative.pMatrix)
# Add it to the classification list
classified<-c(classified,ifelse(positiveProbability>negativeProbability,"positive","negative"))
classprobpos <- c(classprobpos, positiveProbability)
classprobneg <- c(classprobneg, negativeProbability)
}
resultprob <- cbind(tweetsTest,PositiveProbability=classprobpos,NegativeProbability=classprobneg)
View(resultprob)
resultprob <- cbind(CleanTweet=tweetsTest,PositiveProbability=classprobpos,NegativeProbability=classprobneg)
write.csv(resultprob, file=paste('probclassresult.csv'))
resultprob <- cbind(CleanTweet=tweetsTest,PositiveProbability=classprobpos,NegativeProbability=classprobneg,classified)
write.csv(resultprob, file=paste('probclassresult.csv'))
shiny::runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter',port = 6095)
runApp('SkripsiTwitter',port = 6105)
clean_text = function(x, topic)
{
#tolower function often return error on certain character, need try catch
try.error = function(z) #To convert the text in lowercase
{
y = NA
try_error = tryCatch(tolower(z), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(z)
return(y)
}
#case folding
x = gsub("(\r\n)+|\r+|\n+", " ", x) # remove line break
x = gsub("@\\w+", "", x) # remove at(@)
x = gsub("&[^ ]*", "", x) # remove other format (&amp)
x = gsub("http[^ ]*", "", x)  # remove http
x = iconv(x, "latin1", "ASCII", sub="") # remove emoticon
x = gsub("<[^ ]*>", "", x) # remove other format (<564+> emoticon bug)
x = gsub("[[:punct:]]", " ", x) # remove punctuation
x = gsub("[[:digit:]]", "", x) # remove numbers/Digits
x = sapply(x, try.error) #lower case
x = gsub("rt ", "", x) # remove Retweet tag
x = gsub(topic, "", x) # remove main topic term
#filtering
cStopwordID = readLines("stopword-ID.txt") #read stopword file
corpus = Corpus(VectorSource(x)) #tm package only support data type corpus
x = tm_map(corpus, removeWords, c(stopwords("english"),cStopwordID))
#case folding
x = gsub("[ |\t]{2,}", " ", x) # remove tabs
x = gsub("^ ", "", x)  # remove blank spaces at the beginning
x = gsub(" $", "", x) # remove blank spaces at the end
#tokenizing
token = str_split(x, '\\s+')
size = as.numeric(length(x))
for(i in 1:size){
token[[i]] = sapply(token[[i]], katadasaR)
token[[i]] = str_c(token[[i]],collapse=' ') #untokenize
}
return(token)
}
library(devtools)
library(httr)
library(twitteR)
library(tm)
library(katadasaR)
library(stringr)
searchterm <- "machung" #temp for debug
filenameNegative <- c(searchterm,"TrainNegative.csv")
filenameNegative <- str_c(filenameNegative,collapse='')
filenamePositive <- c(searchterm,"TrainPositive.csv")
filenamePositive <- str_c(filenamePositive,collapse='')
filenameTest <- c(searchterm,"DataTestRaw.csv") #temp for debug
filenameTest <- str_c(filenameTest,collapse='')
# Collect data
tweetsTrain.positive<-read.csv(filenamePositive,header=T)
tweetsTrain.positive$Tweet<- clean_text(tweetsTrain.positive$Tweet, searchterm)
tweetsTrain.negative<-read.csv(filenameNegative,header=T)
tweetsTrain.negative$Tweet<- clean_text(tweetsTrain.negative$Tweet, searchterm)
tweetsTestRaw<-read.csv(filenameTest,header=T)
tweetsTest<-clean_text(tweetsTestRaw$text, searchterm)
tweetsTrain.positive["class"]<-rep("positif",nrow(tweetsTrain.positive))
tweetsTrain.negative["class"]<-rep("negatif",nrow(tweetsTrain.negative))
# Create corpus
tweetsTrain.positive.corpus <- Corpus(VectorSource(as.vector(tweetsTrain.positive$Tweet)))
tweetsTrain.negative.corpus <- Corpus(VectorSource(as.vector(tweetsTrain.negative$Tweet)))
tweetsTest.corpus <- Corpus(VectorSource(as.vector(tweetsTest)))
# Create term document matrix
tweetsTrain.positive.matrix <- t(TermDocumentMatrix(tweetsTrain.positive.corpus,control = list(wordLengths=c(3,Inf)))); #exclude word less than 3 character
tweetsTrain.negative.matrix <- t(TermDocumentMatrix(tweetsTrain.negative.corpus,control = list(wordLengths=c(3,Inf))));
tweetsTest.matrix <- t(TermDocumentMatrix(tweetsTest.corpus,control = list(wordLengths=c(3,Inf))));
# Probability Matrix
probabilityMatrix <-function(docMatrix)
{
# Sum up the term frequencies
termSums=cbind(colnames(as.matrix(docMatrix)),as.numeric(colSums(as.matrix(docMatrix))))
# Add one
termSums=cbind(termSums,as.numeric(termSums[,2])+1)
# Calculate the probabilties
termSums=cbind(termSums,(as.numeric(termSums[,3])/sum(as.numeric(termSums[,3]))))
# Calculate the natural log of the probabilities
termSums=cbind(termSums,log(as.numeric(termSums[,4])))
# Add pretty names to the columns
colnames(termSums)=c("term","count","additive","probability","lnProbability")
termSums
}
tweetsTrain.positive.pMatrix<-probabilityMatrix(tweetsTrain.positive.matrix)
tweetsTrain.negative.pMatrix<-probabilityMatrix(tweetsTrain.negative.matrix)
#Predict
# Get the test matrix
# Get words in the first document
getProbability <- function(testChars,probabilityMatrix)
{
charactersFound=probabilityMatrix[probabilityMatrix[,1] %in% testChars,"term"] #in is a matching function
# Count how many words were not found in the positive matrix
charactersNotFound=length(testChars)-length(charactersFound)
# Add the normalized probabilities for the words founds together
charactersFoundSum=sum(as.numeric(probabilityMatrix[probabilityMatrix[,1] %in% testChars,"lnProbability"]))
# We use ln(1/total smoothed words) for words not found
charactersNotFoundSum=charactersNotFound*log(1/sum(as.numeric(probabilityMatrix[,"additive"])))
#This is our probability
prob=charactersFoundSum+charactersNotFoundSum
prob
}
# Get the matrix
tweetsTest.matrix<-as.matrix(tweetsTest.matrix)
# A holder for classification
classified<-NULL
#classprobpos<-NULL #temp for testing
#classprobneg<-NULL #temp for testing
for(documentNumber in 1:nrow(tweetsTest.matrix))
{
# Extract the test words
tweetsTest.chars<-names(tweetsTest.matrix[documentNumber,tweetsTest.matrix[documentNumber,] >= 1])
# Get the probabilities
positiveProbability <- getProbability(tweetsTest.chars,tweetsTrain.positive.pMatrix)
negativeProbability <- getProbability(tweetsTest.chars,tweetsTrain.negative.pMatrix)
# Add it to the classification list
classified<-c(classified,ifelse(positiveProbability>negativeProbability,"positive","negative"))
#classprobpos <- c(classprobpos, positiveProbability)
#classprobneg <- c(classprobneg, negativeProbability)
}
#resultprob <- cbind(CleanTweet=tweetsTest,PositiveProbability=classprobpos,NegativeProbability=classprobneg,classified)
resultRaw <- cbind(tweetsTestRaw,classified)
resultClean <-cbind(text=tweetsTest,classified)
resultnegative <- resultClean[resultClean[, "classified"] == 'negative',"text"]
resultpositive <- resultClean[resultClean[, "classified"] == 'positive',"text"]
result <- list(raw=resultRaw,negative=resultnegative,positive=resultpositive)
View(resultClean)
save.image("~/R Working Directory/.RData")
#read
#searchterm <- "machung"
#minimumfreq <- 2
sparsethreshold <- 0.95
ncomp <- 3
#maxcomp <- 10
#filenameTest <- c(searchterm,"DataTestRaw.csv") #temp for debug
#filenameTest <- str_c(filenameTest,collapse='')
#tweetsTestRaw<-read.csv(filenameTest,header=T)
#tweetsTest<-clean_text(tweetsTestRaw$text)
#tweetsTest.corpus <- Corpus(VectorSource(as.vector(tweetsTest)))
tweetsTest<-resultpositive
nclus = ncomp
tweetsTest.corpus = Corpus(VectorSource(as.vector(tweetsTest)))
#tweetsTest.matrix = t(TermDocumentMatrix(tweetsTest.corpus,control = list(wordLengths=c(3,Inf),bounds = list(global = c(minimumfreq,Inf)))));
tweetsTest.matrix = t(TermDocumentMatrix(tweetsTest.corpus,control = list(wordLengths=c(3,Inf))));
tweetsTest.matrix =removeSparseTerms(tweetsTest.matrix, sparsethreshold)
#remove empty observer
ui = unique(tweetsTest.matrix$i)
tweetsTest.matrix = tweetsTest.matrix[ui,]
termSums=cbind(terms=colnames(as.matrix(tweetsTest.matrix)),frequency=as.numeric(colSums(as.matrix(tweetsTest.matrix))))
#tdm=as.data.frame(as.matrix(tweetsTest.matrix)) #checking result
d = as.matrix(tweetsTest.matrix)
d = t(d) #transpose
pca_existing= prcomp(d, scale=T, center=T)
pca_existing$x[,1:ncomp]
write.csv(pca_existing$x[,1:ncomp], file=paste('pcaresult.csv'))
rawLoadings     = pca_existing$rotation[,1:ncomp] %*% diag(pca_existing$sdev, ncomp, ncomp)
scores = scale(pca_existing$x[,1:ncomp]) %*% varimax(rawLoadings)$rotmat     #scale is for standarize
#print(scores[1:5,])
scores
data_clustered = kmeans(scores,nclus)
#scores = cbind(cluster=data_clustered$cluster,scores)
result = cbind(termSums,cluster=data_clustered$cluster)
rownames(result) = NULL
write.csv(result, file=paste('PCAresult.csv'))
write.csv(result, file=paste('PCAresult.csv'))
scores = cbind(cluster=data_clustered$cluster,scores)
scores
write.csv(scores, file=paste('PCAresult.csv'))
shiny::runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
shiny::runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter',port = 6105)
library(devtools)
library(httr)
library(twitteR)
library(tm)
library(katadasaR)
library(stringr)
#library(psych)
searchterm <- "machung" #temp for debug
filenameNegative <- c(searchterm,"TrainNegative.csv")
filenameNegative <- str_c(filenameNegative,collapse='')
filenamePositive <- c(searchterm,"TrainPositive.csv")
filenamePositive <- str_c(filenamePositive,collapse='')
filenameTest <- c(searchterm,"DataTestRaw.csv") #temp for debug
filenameTest <- str_c(filenameTest,collapse='')
# Collect data
tweetsTrain.positive<-read.csv(filenamePositive,header=T)
tweetsTrain.positive$Tweet<- clean_text(tweetsTrain.positive$Tweet, searchterm)
tweetsTrain.negative<-read.csv(filenameNegative,header=T)
tweetsTrain.negative$Tweet<- clean_text(tweetsTrain.negative$Tweet, searchterm)
tweetsTestRaw<-read.csv(filenameTest,header=T)
tweetsTest<-clean_text(tweetsTestRaw$text, searchterm)
tweetsTrain.positive["class"]<-rep("positif",nrow(tweetsTrain.positive))
tweetsTrain.negative["class"]<-rep("negatif",nrow(tweetsTrain.negative))
# Create corpus
tweetsTrain.positive.corpus <- Corpus(VectorSource(as.vector(tweetsTrain.positive$Tweet)))
tweetsTrain.negative.corpus <- Corpus(VectorSource(as.vector(tweetsTrain.negative$Tweet)))
tweetsTest.corpus <- Corpus(VectorSource(as.vector(tweetsTest)))
# Create term document matrix
tweetsTrain.positive.matrix <- t(TermDocumentMatrix(tweetsTrain.positive.corpus,control = list(wordLengths=c(3,Inf)))); #exclude word less than 3 character
tweetsTrain.negative.matrix <- t(TermDocumentMatrix(tweetsTrain.negative.corpus,control = list(wordLengths=c(3,Inf))));
tweetsTest.matrix <- t(TermDocumentMatrix(tweetsTest.corpus,control = list(wordLengths=c(3,Inf))));
# Probability Matrix
probabilityMatrix <-function(docMatrix)
{
# Sum up the term frequencies
termSums=cbind(colnames(as.matrix(docMatrix)),as.numeric(colSums(as.matrix(docMatrix))))
# Add one
termSums=cbind(termSums,as.numeric(termSums[,2])+1)
# Calculate the probabilties
termSums=cbind(termSums,(as.numeric(termSums[,3])/sum(as.numeric(termSums[,3]))))
# Calculate the natural log of the probabilities
termSums=cbind(termSums,log(as.numeric(termSums[,4])))
# Add pretty names to the columns
colnames(termSums)=c("term","count","additive","probability","lnProbability")
termSums
}
tweetsTrain.positive.pMatrix<-probabilityMatrix(tweetsTrain.positive.matrix)
tweetsTrain.negative.pMatrix<-probabilityMatrix(tweetsTrain.negative.matrix)
getProbability <- function(testChars,probabilityMatrix)
{
charactersFound=probabilityMatrix[probabilityMatrix[,1] %in% testChars,"term"] #in is a matching function
# Count how many words were not found in the positive matrix
charactersNotFound=length(testChars)-length(charactersFound)
# Add the normalized probabilities for the words founds together
charactersFoundSum=sum(as.numeric(probabilityMatrix[probabilityMatrix[,1] %in% testChars,"lnProbability"]))
# We use ln(1/total smoothed words) for words not found
charactersNotFoundSum=charactersNotFound*log(1/sum(as.numeric(probabilityMatrix[,"additive"])))
#This is our probability
prob=charactersFoundSum+charactersNotFoundSum
prob
}
# Get the matrix
tweetsTest.matrix<-as.matrix(tweetsTest.matrix)
# A holder for classification
classified<-NULL
#classprobpos<-NULL #temp for testing
#classprobneg<-NULL #temp for testing
for(documentNumber in 1:nrow(tweetsTest.matrix))
{
# Extract the test words
tweetsTest.chars<-names(tweetsTest.matrix[documentNumber,tweetsTest.matrix[documentNumber,] >= 1])
# Get the probabilities
positiveProbability <- getProbability(tweetsTest.chars,tweetsTrain.positive.pMatrix)
negativeProbability <- getProbability(tweetsTest.chars,tweetsTrain.negative.pMatrix)
# Add it to the classification list
classified<-c(classified,ifelse(positiveProbability>negativeProbability,"positive","negative"))
#classprobpos <- c(classprobpos, positiveProbability)
#classprobneg <- c(classprobneg, negativeProbability)
}
#resultprob <- cbind(CleanTweet=tweetsTest,PositiveProbability=classprobpos,NegativeProbability=classprobneg,classified)
resultRaw <- cbind(tweetsTestRaw,classified)
resultRawnegative <- resultRaw[resultClean[, "classified"] == 'negative',c("created","text")]
resultRawpositive <- resultRaw[resultClean[, "classified"] == 'positive',c("created","text")]
resultClean <-cbind(text=tweetsTest,classified)
resultnegative <- resultClean[resultClean[, "classified"] == 'negative',"text"]
resultpositive <- resultClean[resultClean[, "classified"] == 'positive',"text"]
View(resultRawnegative)
View(resultRawpositive)
result <- list(rawnegative=resultRawnegative,rawpositive=resultRawpositive,negative=resultnegative,positive=resultpositive)
result
result$rawnegative
shiny::runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
View(tweetsTrain.negative)
runApp('SkripsiTwitter',port = 6105)
runApp('SkripsiTwitter',port = 6105)
shiny::runApp('SkripsiTwitter')
library(devtools)
library(httr)
library(twitteR)
library(tm)
library(katadasaR)
library(stringr)
#library(psych)
library(shiny)
runApp('SkripsiTwitter',port = 6105)
runApp('SkripsiTwitter',port = 6105)
shiny::runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter')
runApp('SkripsiTwitter',port = 6105)
shiny::runApp('SkripsiTwitter')
